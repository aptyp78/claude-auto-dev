"""
Execution Scheduler

–í—ã–ø–æ–ª–Ω—è–µ—Ç –∑–∞–¥–∞—á–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –∏–ª–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ,
—É–ø—Ä–∞–≤–ª—è–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –∏ –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç—Å—è —Å LLM.
"""

import asyncio
import json
import time
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
from typing import Dict, List, Optional, Callable, Any
from concurrent.futures import ThreadPoolExecutor
import httpx

from .task_parser import Task, TaskGraph, TaskStatus, TaskType
from .model_router import ModelRouter, ModelConfig
from .agent_router import AgentRouter, AgentConfig, AgentType


class ExecutionMode(Enum):
    """–†–µ–∂–∏–º—ã –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è"""
    SEQUENTIAL = "sequential"    # –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ
    PARALLEL = "parallel"        # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –≥–¥–µ –≤–æ–∑–º–æ–∂–Ω–æ
    INTERACTIVE = "interactive"  # –° –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è–º–∏


@dataclass
class ExecutionResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á–∏"""
    task_id: str
    success: bool
    output: str = ""
    error: Optional[str] = None

    # –ú–µ—Ç—Ä–∏–∫–∏
    tokens_used: int = 0
    execution_time_seconds: float = 0
    model_used: str = ""
    agent_used: str = ""

    # –ê—Ä—Ç–µ—Ñ–∞–∫—Ç—ã
    files_created: List[str] = field(default_factory=list)
    files_modified: List[str] = field(default_factory=list)

    def to_dict(self) -> dict:
        return {
            "task_id": self.task_id,
            "success": self.success,
            "output": self.output[:500] if self.output else "",
            "error": self.error,
            "tokens_used": self.tokens_used,
            "execution_time": self.execution_time_seconds,
            "model": self.model_used,
            "agent": self.agent_used,
        }


@dataclass
class ExecutionContext:
    """–ö–æ–Ω—Ç–µ–∫—Å—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è"""
    task: Task
    agent: AgentConfig
    model: ModelConfig

    # RAG –∫–æ–Ω—Ç–µ–∫—Å—Ç
    relevant_code: str = ""
    project_memories: str = ""

    # –ò—Å—Ç–æ—Ä–∏—è
    previous_results: List[ExecutionResult] = field(default_factory=list)

    # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è
    max_tokens: int = 32000
    timeout_seconds: int = 300


class LLMClient:
    """–ö–ª–∏–µ–Ω—Ç –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å Ollama"""

    def __init__(self, base_url: str = "http://localhost:11434"):
        self.base_url = base_url
        self.client = httpx.Client(timeout=300.0)

    def generate(
        self,
        prompt: str,
        model: str,
        system: str = "",
        temperature: float = 0.3,
        max_tokens: int = 4096,
    ) -> Dict[str, Any]:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ Ollama API

        Returns:
            dict: {"response": str, "tokens": int, "time": float}
        """
        start_time = time.time()

        messages = []
        if system:
            messages.append({"role": "system", "content": system})
        messages.append({"role": "user", "content": prompt})

        try:
            response = self.client.post(
                f"{self.base_url}/api/chat",
                json={
                    "model": model,
                    "messages": messages,
                    "stream": False,
                    "options": {
                        "temperature": temperature,
                        "num_predict": max_tokens,
                    }
                }
            )
            response.raise_for_status()
            data = response.json()

            elapsed = time.time() - start_time

            return {
                "response": data.get("message", {}).get("content", ""),
                "tokens": data.get("eval_count", 0) + data.get("prompt_eval_count", 0),
                "time": elapsed,
                "model": model,
            }

        except Exception as e:
            return {
                "response": "",
                "error": str(e),
                "tokens": 0,
                "time": time.time() - start_time,
                "model": model,
            }

    def check_health(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å Ollama"""
        try:
            response = self.client.get(f"{self.base_url}/api/tags")
            return response.status_code == 200
        except:
            return False


class ExecutionScheduler:
    """
    –ü–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á

    –£–ø—Ä–∞–≤–ª—è–µ—Ç:
    - –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–º –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ–º –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã—Ö –∑–∞–¥–∞—á
    - –í—ã–±–æ—Ä–æ–º –º–æ–¥–µ–ª–µ–π –∏ –∞–≥–µ–Ω—Ç–æ–≤
    - –°–±–æ—Ä–∫–æ–π –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
    - Retry –ª–æ–≥–∏–∫–æ–π
    """

    def __init__(
        self,
        ollama_url: str = "http://localhost:11434",
        max_parallel: int = 2,
        max_retries: int = 3,
        mode: ExecutionMode = ExecutionMode.PARALLEL
    ):
        self.llm = LLMClient(ollama_url)
        self.model_router = ModelRouter(ollama_url)
        self.agent_router = AgentRouter()

        self.max_parallel = max_parallel
        self.max_retries = max_retries
        self.mode = mode

        self.executor = ThreadPoolExecutor(max_workers=max_parallel)

        # Callbacks
        self.on_task_start: Optional[Callable[[Task], None]] = None
        self.on_task_complete: Optional[Callable[[Task, ExecutionResult], None]] = None
        self.on_human_approval: Optional[Callable[[str], bool]] = None

        # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã
        self.results: Dict[str, ExecutionResult] = {}

    def _build_prompt(
        self,
        context: ExecutionContext
    ) -> str:
        """–ü–æ—Å—Ç—Ä–æ–∏—Ç—å –ø—Ä–æ–º–ø—Ç –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á–∏"""
        parts = []

        # –û–ø–∏—Å–∞–Ω–∏–µ –∑–∞–¥–∞—á–∏
        parts.append(f"## –ó–∞–¥–∞—á–∞: {context.task.title}")
        parts.append(f"\n{context.task.description}")

        # –§–∞–π–ª—ã –¥–ª—è —Ä–∞–±–æ—Ç—ã
        if context.task.files_to_read:
            parts.append(f"\n## –§–∞–π–ª—ã –¥–ª—è —á—Ç–µ–Ω–∏—è:\n- " + "\n- ".join(context.task.files_to_read))

        if context.task.files_to_modify:
            parts.append(f"\n## –§–∞–π–ª—ã –¥–ª—è –∏–∑–º–µ–Ω–µ–Ω–∏—è:\n- " + "\n- ".join(context.task.files_to_modify))

        # RAG –∫–æ–Ω—Ç–µ–∫—Å—Ç
        if context.relevant_code:
            parts.append(f"\n## –†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –∫–æ–¥:\n```\n{context.relevant_code}\n```")

        # –ü–∞–º—è—Ç—å –ø—Ä–æ–µ–∫—Ç–∞
        if context.project_memories:
            parts.append(f"\n## –ü–∞–º—è—Ç—å –ø—Ä–æ–µ–∫—Ç–∞:\n{context.project_memories}")

        # –ü—Ä–µ–¥—ã–¥—É—â–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        if context.previous_results:
            parts.append("\n## –ü—Ä–µ–¥—ã–¥—É—â–∏–µ —à–∞–≥–∏:")
            for result in context.previous_results[-3:]:  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 3
                status = "‚úÖ" if result.success else "‚ùå"
                parts.append(f"- {status} {result.task_id}: {result.output[:200]}...")

        return "\n".join(parts)

    def _execute_single_task(
        self,
        task: Task,
        context: ExecutionContext
    ) -> ExecutionResult:
        """–í—ã–ø–æ–ª–Ω–∏—Ç—å –æ–¥–Ω—É –∑–∞–¥–∞—á—É"""
        start_time = time.time()

        # Callback: –Ω–∞—á–∞–ª–æ
        if self.on_task_start:
            self.on_task_start(task)

        # –ü–æ—Å—Ç—Ä–æ–∏—Ç—å –ø—Ä–æ–º–ø—Ç
        prompt = self._build_prompt(context)

        # –í—ã–∑–≤–∞—Ç—å LLM
        llm_result = self.llm.generate(
            prompt=prompt,
            model=context.model.name,
            system=context.agent.system_prompt,
            temperature=context.agent.temperature,
            max_tokens=context.agent.max_tokens,
        )

        elapsed = time.time() - start_time

        # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –Ω–∞ –æ—à–∏–±–∫—É
        if "error" in llm_result:
            result = ExecutionResult(
                task_id=task.id,
                success=False,
                error=llm_result["error"],
                execution_time_seconds=elapsed,
                model_used=context.model.name,
                agent_used=context.agent.name,
            )
        else:
            result = ExecutionResult(
                task_id=task.id,
                success=True,
                output=llm_result["response"],
                tokens_used=llm_result["tokens"],
                execution_time_seconds=elapsed,
                model_used=context.model.name,
                agent_used=context.agent.name,
            )

        # Callback: –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ
        if self.on_task_complete:
            self.on_task_complete(task, result)

        return result

    def _execute_with_retry(
        self,
        task: Task,
        project_context: str = "",
        rag_context: str = ""
    ) -> ExecutionResult:
        """–í—ã–ø–æ–ª–Ω–∏—Ç—å –∑–∞–¥–∞—á—É —Å retry –ª–æ–≥–∏–∫–æ–π"""
        # –í—ã–±—Ä–∞—Ç—å –∞–≥–µ–Ω—Ç–∞
        agent = self.agent_router.select_agent(
            task.type.value,
            task.description
        )

        # –í—ã–±—Ä–∞—Ç—å –º–æ–¥–µ–ª—å
        model, reason = self.model_router.select_model(
            task.type.value,
            complexity="medium"
        )

        # –°–æ–∑–¥–∞—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç
        context = ExecutionContext(
            task=task,
            agent=agent,
            model=model,
            relevant_code=rag_context,
            project_memories=project_context,
        )

        last_error = None
        for attempt in range(self.max_retries):
            result = self._execute_single_task(task, context)

            if result.success:
                return result

            last_error = result.error
            print(f"  ‚ö†Ô∏è Attempt {attempt + 1} failed: {last_error}")

            # –î–ª—è retry –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥—Ä—É–≥–æ–≥–æ –∞–≥–µ–Ω—Ç–∞ –∏–ª–∏ –º–æ–¥–µ–ª—å
            if attempt == 1:
                # –ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å fallback –º–æ–¥–µ–ª—å
                fallback_model = self.model_router.models.get(agent.fallback_model)
                if fallback_model:
                    context.model = fallback_model
                    print(f"  üîÑ Switching to fallback model: {fallback_model.name}")

            elif attempt == 2:
                # –ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –¥—Ä—É–≥–æ–≥–æ –∞–≥–µ–Ω—Ç–∞
                retry_agent = self.agent_router.get_agent_for_retry(
                    agent.type,
                    last_error or ""
                )
                context.agent = retry_agent
                print(f"  üîÑ Switching to retry agent: {retry_agent.name}")

        # –í—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –∏—Å—á–µ—Ä–ø–∞–Ω—ã
        return ExecutionResult(
            task_id=task.id,
            success=False,
            error=f"All {self.max_retries} attempts failed. Last error: {last_error}",
            model_used=context.model.name,
            agent_used=context.agent.name,
        )

    def execute_task(
        self,
        task: Task,
        project_context: str = "",
        rag_context: str = ""
    ) -> ExecutionResult:
        """
        –í—ã–ø–æ–ª–Ω–∏—Ç—å –æ–¥–Ω—É –∑–∞–¥–∞—á—É

        Args:
            task: –ó–∞–¥–∞—á–∞ –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
            project_context: –ö–æ–Ω—Ç–µ–∫—Å—Ç –ø—Ä–æ–µ–∫—Ç–∞
            rag_context: –ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ RAG

        Returns:
            ExecutionResult
        """
        task.status = TaskStatus.IN_PROGRESS
        task.started_at = datetime.now()

        result = self._execute_with_retry(task, project_context, rag_context)

        task.result = result.output if result.success else None
        task.error = result.error
        task.status = TaskStatus.COMPLETED if result.success else TaskStatus.FAILED
        task.completed_at = datetime.now()

        self.results[task.id] = result
        return result

    async def execute_graph_async(
        self,
        graph: TaskGraph,
        project_context: str = "",
        rag_retriever: Optional[Callable[[str], str]] = None
    ) -> Dict[str, ExecutionResult]:
        """
        –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∞ –∑–∞–¥–∞—á

        Args:
            graph: –ì—Ä–∞—Ñ –∑–∞–¥–∞—á
            project_context: –ö–æ–Ω—Ç–µ–∫—Å—Ç –ø—Ä–æ–µ–∫—Ç–∞
            rag_retriever: –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è RAG –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞

        Returns:
            Dict task_id -> ExecutionResult
        """
        completed = set()
        results = {}

        while len(completed) < len(graph.tasks):
            # –ù–∞–π—Ç–∏ –≥–æ—Ç–æ–≤—ã–µ –∑–∞–¥–∞—á–∏
            ready_tasks = graph.get_ready_tasks(completed)

            if not ready_tasks:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ deadlock
                pending = [t for t in graph.tasks.values()
                          if t.id not in completed]
                if pending:
                    print(f"‚ö†Ô∏è Deadlock detected. Pending tasks: {[t.id for t in pending]}")
                    break
                continue

            # –û–≥—Ä–∞–Ω–∏—á–∏—Ç—å –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ—Å—Ç—å
            batch = ready_tasks[:self.max_parallel]

            print(f"\nüì¶ Executing batch of {len(batch)} tasks:")
            for task in batch:
                print(f"   - {task.id}: {task.title}")

            # –í—ã–ø–æ–ª–Ω–∏—Ç—å –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
            if self.mode == ExecutionMode.PARALLEL and len(batch) > 1:
                tasks_coro = []
                for task in batch:
                    # –ü–æ–ª—É—á–∏—Ç—å RAG –∫–æ–Ω—Ç–µ–∫—Å—Ç –µ—Å–ª–∏ –µ—Å—Ç—å retriever
                    rag_context = ""
                    if rag_retriever:
                        rag_context = rag_retriever(task.description)

                    # –°–æ–∑–¥–∞—Ç—å –∫–æ—Ä—É—Ç–∏–Ω—É
                    coro = asyncio.get_event_loop().run_in_executor(
                        self.executor,
                        self._execute_with_retry,
                        task,
                        project_context,
                        rag_context
                    )
                    tasks_coro.append((task, coro))

                # –ñ–¥–∞—Ç—å –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
                for task, coro in tasks_coro:
                    result = await coro
                    results[task.id] = result
                    completed.add(task.id)

                    status = "‚úÖ" if result.success else "‚ùå"
                    print(f"   {status} {task.id}: {result.execution_time_seconds:.1f}s")

            else:
                # –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ
                for task in batch:
                    rag_context = ""
                    if rag_retriever:
                        rag_context = rag_retriever(task.description)

                    result = self.execute_task(task, project_context, rag_context)
                    results[task.id] = result
                    completed.add(task.id)

                    status = "‚úÖ" if result.success else "‚ùå"
                    print(f"   {status} {task.id}: {result.execution_time_seconds:.1f}s")

        self.results = results
        return results

    def execute_graph_sync(
        self,
        graph: TaskGraph,
        project_context: str = "",
        rag_retriever: Optional[Callable[[str], str]] = None
    ) -> Dict[str, ExecutionResult]:
        """
        –ü–æ–ª–Ω–æ—Å—Ç—å—é —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –≥—Ä–∞—Ñ–∞ –∑–∞–¥–∞—á
        –í—ã–ø–æ–ª–Ω—è–µ—Ç –∑–∞–¥–∞—á–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ —Å —É—á—ë—Ç–æ–º –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
        """
        completed = set()
        results = {}

        while len(completed) < len(graph.tasks):
            # –ù–∞–π—Ç–∏ –∑–∞–¥–∞—á–∏, –≥–æ—Ç–æ–≤—ã–µ –∫ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—é
            ready_tasks = [
                task for task in graph.tasks.values()
                if task.id not in completed
                and all(dep in completed for dep in task.depends_on)
            ]

            if not ready_tasks:
                # Deadlock –∏–ª–∏ –≤—Å–µ –≤—ã–ø–æ–ª–Ω–µ–Ω—ã
                remaining = set(graph.tasks.keys()) - completed
                if remaining:
                    print(f"‚ö†Ô∏è Deadlock: {remaining}")
                break

            print(f"\nüì¶ Executing {len(ready_tasks)} task(s):")
            for task in ready_tasks:
                print(f"   - {task.id}: {task.title}")

            # –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ
            for task in ready_tasks:
                rag_context = ""
                if rag_retriever:
                    rag_context = rag_retriever(task.description)

                result = self.execute_task(task, project_context, rag_context)
                results[task.id] = result
                completed.add(task.id)

                status = "‚úÖ" if result.success else "‚ùå"
                print(f"   {status} {task.id}: {result.execution_time_seconds:.1f}s")

        self.results = results
        return results

    def execute_graph(
        self,
        graph: TaskGraph,
        project_context: str = "",
        rag_retriever: Optional[Callable[[str], str]] = None
    ) -> Dict[str, ExecutionResult]:
        """
        –°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –≥—Ä–∞—Ñ–∞ –∑–∞–¥–∞—á
        """
        return self.execute_graph_sync(graph, project_context, rag_retriever)

    def get_summary(self) -> dict:
        """–ü–æ–ª—É—á–∏—Ç—å —Å–≤–æ–¥–∫—É –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è"""
        total = len(self.results)
        successful = sum(1 for r in self.results.values() if r.success)
        failed = total - successful

        total_tokens = sum(r.tokens_used for r in self.results.values())
        total_time = sum(r.execution_time_seconds for r in self.results.values())

        models_used = list(set(r.model_used for r in self.results.values()))
        agents_used = list(set(r.agent_used for r in self.results.values()))

        return {
            "total_tasks": total,
            "successful": successful,
            "failed": failed,
            "success_rate": successful / total if total > 0 else 0,
            "total_tokens": total_tokens,
            "total_time_seconds": total_time,
            "models_used": models_used,
            "agents_used": agents_used,
        }


# CLI –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    from .task_parser import TaskParser

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º Ollama
    scheduler = ExecutionScheduler()

    if not scheduler.llm.check_health():
        print("‚ùå Ollama not running. Please start: ollama serve")
        exit(1)

    print("‚úÖ Ollama is running")

    # –°–æ–∑–¥–∞—ë–º –ø—Ä–æ—Å—Ç—É—é –∑–∞–¥–∞—á—É
    parser = TaskParser()
    graph = parser.create_simple_task(
        "–ù–∞–ø–∏—à–∏ —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ email –∞–¥—Ä–µ—Å–∞ –Ω–∞ Python",
        TaskType.IMPLEMENTATION
    )

    print(f"\nüìã Task: {graph.tasks['task_1'].title}")

    # –í—ã–ø–æ–ª–Ω—è–µ–º
    results = scheduler.execute_graph(graph)

    # –†–µ–∑—É–ª—å—Ç–∞—Ç
    print("\nüìä Results:")
    for task_id, result in results.items():
        status = "‚úÖ" if result.success else "‚ùå"
        print(f"  {status} {task_id}")
        print(f"     Model: {result.model_used}")
        print(f"     Agent: {result.agent_used}")
        print(f"     Time: {result.execution_time_seconds:.1f}s")
        print(f"     Tokens: {result.tokens_used}")

        if result.success:
            print(f"\nüìù Output:\n{result.output[:500]}...")
        else:
            print(f"\n‚ùå Error: {result.error}")

    # –°–≤–æ–¥–∫–∞
    summary = scheduler.get_summary()
    print(f"\nüìà Summary:")
    print(f"   Success rate: {summary['success_rate']*100:.0f}%")
    print(f"   Total tokens: {summary['total_tokens']}")
    print(f"   Total time: {summary['total_time_seconds']:.1f}s")
