"""
Model Router

–í—ã–±–∏—Ä–∞–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è –∫–∞–∂–¥–æ–π –∑–∞–¥–∞—á–∏.
–£—á–∏—Ç—ã–≤–∞–µ—Ç: —Ç–∏–ø –∑–∞–¥–∞—á–∏, —Å–ª–æ–∂–Ω–æ—Å—Ç—å, –¥–æ—Å—Ç—É–ø–Ω—É—é RAM, –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏.
"""

import subprocess
import json
from dataclasses import dataclass, field
from enum import Enum
from typing import Dict, List, Optional, Tuple
import httpx
import psutil


class ModelTier(Enum):
    """–£—Ä–æ–≤–Ω–∏ –º–æ–¥–µ–ª–µ–π –ø–æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
    FAST = "fast"          # <8B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –º–≥–Ω–æ–≤–µ–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã
    BALANCED = "balanced"  # 14-32B, —Ö–æ—Ä–æ—à–∏–π –±–∞–ª–∞–Ω—Å
    POWERFUL = "powerful"  # 70B+, –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ


@dataclass
class ModelConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–∏"""
    name: str
    provider: str = "ollama"
    tier: ModelTier = ModelTier.BALANCED
    ram_gb: float = 0
    context_window: int = 32768

    # –°–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏
    strengths: List[str] = field(default_factory=list)

    # –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
    tokens_per_second: float = 40.0
    first_token_latency: float = 2.0

    # –°—Ç–∞—Ç—É—Å
    is_loaded: bool = False

    def __post_init__(self):
        if not self.strengths:
            self.strengths = ["general"]


# –ü—Ä–µ–¥–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏
AVAILABLE_MODELS: Dict[str, ModelConfig] = {
    # Fast tier
    "qwen3:8b": ModelConfig(
        name="qwen3:8b",
        tier=ModelTier.FAST,
        ram_gb=5.2,
        strengths=["classification", "summarization", "quick_tasks"],
        tokens_per_second=80,
        first_token_latency=0.5
    ),

    # Balanced tier - Coding
    "qwen3-coder:30b": ModelConfig(
        name="qwen3-coder:30b",
        tier=ModelTier.BALANCED,
        ram_gb=18,
        context_window=32768,
        strengths=["implementation", "refactoring", "testing", "debugging"],
        tokens_per_second=50,
        first_token_latency=1.5
    ),
    "deepseek-coder:33b-instruct": ModelConfig(
        name="deepseek-coder:33b-instruct",
        tier=ModelTier.BALANCED,
        ram_gb=18,
        strengths=["implementation", "code_generation"],
        tokens_per_second=45,
        first_token_latency=2.0
    ),
    "codestral:latest": ModelConfig(
        name="codestral:latest",
        tier=ModelTier.BALANCED,
        ram_gb=12,
        strengths=["fim", "autocomplete", "snippets"],
        tokens_per_second=55,
        first_token_latency=1.0
    ),
    "devstral": ModelConfig(
        name="devstral",
        tier=ModelTier.BALANCED,
        ram_gb=14,
        strengths=["agentic", "swe_tasks", "multi_step"],
        tokens_per_second=45,
        first_token_latency=1.5
    ),

    # Balanced tier - Reasoning
    "deepseek-r1:32b": ModelConfig(
        name="deepseek-r1:32b",
        tier=ModelTier.BALANCED,
        ram_gb=19,
        strengths=["architecture", "review", "reasoning", "debugging", "planning"],
        tokens_per_second=35,
        first_token_latency=2.5
    ),
    "codellama:34b": ModelConfig(
        name="codellama:34b",
        tier=ModelTier.BALANCED,
        ram_gb=19,
        strengths=["review", "analysis", "code_understanding"],
        tokens_per_second=40,
        first_token_latency=2.0
    ),

    # Powerful tier
    "llama3.3:latest": ModelConfig(
        name="llama3.3:latest",
        tier=ModelTier.POWERFUL,
        ram_gb=42,
        context_window=131072,
        strengths=["general", "long_context", "complex_tasks"],
        tokens_per_second=25,
        first_token_latency=3.0
    ),
    "llama3:70b": ModelConfig(
        name="llama3:70b",
        tier=ModelTier.POWERFUL,
        ram_gb=39,
        strengths=["complex_reasoning", "fallback"],
        tokens_per_second=20,
        first_token_latency=4.0
    ),

    # Embedding models
    "nomic-embed-text": ModelConfig(
        name="nomic-embed-text",
        tier=ModelTier.FAST,
        ram_gb=0.3,
        strengths=["embeddings", "semantic_search"],
        tokens_per_second=300,
        first_token_latency=0.01
    ),
    "qwen3-embedding:8b": ModelConfig(
        name="qwen3-embedding:8b",
        tier=ModelTier.BALANCED,
        ram_gb=4.7,
        strengths=["embeddings", "code_embeddings"],
        tokens_per_second=200,
        first_token_latency=0.05
    ),
}

# –ú–∞–ø–ø–∏–Ω–≥ –∑–∞–¥–∞—á –Ω–∞ –º–æ–¥–µ–ª–∏
TASK_MODEL_MAPPING = {
    # –¢–∏–ø –∑–∞–¥–∞—á–∏ -> (primary_model, fallback_model)
    "architecture": ("deepseek-r1:32b", "llama3:70b"),
    "implementation": ("qwen3-coder:30b", "deepseek-coder:33b-instruct"),
    "refactoring": ("qwen3-coder:30b", "deepseek-r1:32b"),
    "bugfix": ("qwen3-coder:30b", "deepseek-r1:32b"),
    "testing": ("qwen3-coder:30b", "devstral"),
    "review": ("deepseek-r1:32b", "codellama:34b"),
    "documentation": ("qwen3:8b", "qwen3-coder:30b"),
    "devops": ("qwen3-coder:30b", "deepseek-r1:32b"),
    "research": ("deepseek-r1:32b", "llama3.3:latest"),
    "agentic": ("devstral", "qwen3-coder:30b"),
    "fim": ("codestral:latest", "qwen3-coder:30b"),
    "quick": ("qwen3:8b", "qwen3-coder:30b"),
    "embedding": ("nomic-embed-text", "qwen3-embedding:8b"),
}


class ModelRouter:
    """
    –†–æ—É—Ç–µ—Ä –¥–ª—è –≤—ã–±–æ—Ä–∞ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏
    """

    def __init__(
        self,
        ollama_url: str = "http://localhost:11434",
        max_ram_gb: float = 98.0,  # 128 - 30 (OS + services)
        prefer_speed: bool = False
    ):
        self.ollama_url = ollama_url
        self.max_ram_gb = max_ram_gb
        self.prefer_speed = prefer_speed
        self.client = httpx.Client(timeout=30.0)

        self.models = AVAILABLE_MODELS.copy()
        self.loaded_models: Dict[str, ModelConfig] = {}

        # –û–±–Ω–æ–≤–∏—Ç—å —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
        self._refresh_available_models()

    def _refresh_available_models(self) -> None:
        """–û–±–Ω–æ–≤–∏—Ç—å —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π –∏–∑ Ollama"""
        try:
            result = subprocess.run(
                ["ollama", "list"],
                capture_output=True,
                text=True,
                timeout=10
            )

            if result.returncode == 0:
                lines = result.stdout.strip().split("\n")[1:]  # Skip header

                available = set()
                for line in lines:
                    if line.strip():
                        name = line.split()[0]
                        available.add(name)

                # –û–±–Ω–æ–≤–∏—Ç—å —Å—Ç–∞—Ç—É—Å –º–æ–¥–µ–ª–µ–π
                for model_name in self.models:
                    if model_name in available or model_name.split(":")[0] in available:
                        self.models[model_name].is_loaded = True

        except Exception as e:
            print(f"Warning: Could not refresh models: {e}")

    def _get_available_ram(self) -> float:
        """–ü–æ–ª—É—á–∏—Ç—å –¥–æ—Å—Ç—É–ø–Ω—É—é RAM –≤ GB"""
        mem = psutil.virtual_memory()
        available_gb = mem.available / (1024 ** 3)
        return min(available_gb, self.max_ram_gb)

    def _get_loaded_ram(self) -> float:
        """–ü–æ–ª—É—á–∏—Ç—å RAM, –∏—Å–ø–æ–ª—å–∑—É–µ–º—É—é –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏"""
        return sum(m.ram_gb for m in self.loaded_models.values())

    def select_model(
        self,
        task_type: str,
        complexity: str = "medium",
        context_size: int = 0,
        force_model: Optional[str] = None
    ) -> Tuple[ModelConfig, str]:
        """
        –í—ã–±—Ä–∞—Ç—å –æ–ø—Ç–∏–º–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è –∑–∞–¥–∞—á–∏

        Args:
            task_type: –¢–∏–ø –∑–∞–¥–∞—á–∏ (architecture, implementation, etc.)
            complexity: –°–ª–æ–∂–Ω–æ—Å—Ç—å (simple, medium, complex)
            context_size: –ü—Ä–∏–º–µ—Ä–Ω—ã–π —Ä–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤ —Ç–æ–∫–µ–Ω–∞—Ö
            force_model: –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —ç—Ç—É –º–æ–¥–µ–ª—å

        Returns:
            Tuple[ModelConfig, str]: (–∫–æ–Ω—Ñ–∏–≥ –º–æ–¥–µ–ª–∏, –ø—Ä–∏—á–∏–Ω–∞ –≤—ã–±–æ—Ä–∞)
        """
        # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–π –≤—ã–±–æ—Ä
        if force_model and force_model in self.models:
            return self.models[force_model], f"Forced: {force_model}"

        # –ü–æ–ª—É—á–∏—Ç—å —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —Ç–∏–ø–∞ –∑–∞–¥–∞—á–∏
        primary, fallback = TASK_MODEL_MAPPING.get(
            task_type,
            ("qwen3-coder:30b", "deepseek-r1:32b")
        )

        # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å primary –º–æ–¥–µ–ª–∏
        primary_config = self.models.get(primary)
        if not primary_config:
            primary_config = self.models.get(fallback, list(self.models.values())[0])

        # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å RAM
        available_ram = self._get_available_ram()

        # –ï—Å–ª–∏ —Å–ª–æ–∂–Ω–∞—è –∑–∞–¥–∞—á–∞ –∏ –µ—Å—Ç—å RAM ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º –º–æ—â–Ω—É—é –º–æ–¥–µ–ª—å
        if complexity == "complex" and available_ram > 40:
            # –ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –±–æ–ª–µ–µ –º–æ—â–Ω—É—é –º–æ–¥–µ–ª—å
            if task_type in ["architecture", "research", "review"]:
                if "llama3:70b" in self.models and available_ram > 40:
                    return self.models["llama3:70b"], "Complex task, using powerful model"

        # –ï—Å–ª–∏ –ø—Ä–æ—Å—Ç–∞—è –∑–∞–¥–∞—á–∞ –∏–ª–∏ prefer_speed ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º –±—ã—Å—Ç—Ä—É—é
        if complexity == "simple" or self.prefer_speed:
            fast_models = [m for m in self.models.values() if m.tier == ModelTier.FAST]
            if fast_models:
                return fast_models[0], "Simple task, using fast model"

        # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç
        if context_size > 32000 and "llama3.3:latest" in self.models:
            return self.models["llama3.3:latest"], "Large context, using 128K model"

        # –í–µ—Ä–Ω—É—Ç—å primary –º–æ–¥–µ–ª—å
        reason = f"Best fit for {task_type}"
        return primary_config, reason

    def get_embedding_model(self) -> ModelConfig:
        """–ü–æ–ª—É—á–∏—Ç—å –º–æ–¥–µ–ª—å –¥–ª—è embeddings"""
        if "nomic-embed-text" in self.models:
            return self.models["nomic-embed-text"]
        return self.models.get("qwen3-embedding:8b", list(self.models.values())[0])

    def can_load_model(self, model_name: str) -> Tuple[bool, str]:
        """
        –ü—Ä–æ–≤–µ—Ä–∏—Ç—å, –º–æ–∂–Ω–æ –ª–∏ –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å

        Returns:
            Tuple[bool, str]: (–º–æ–∂–Ω–æ –ª–∏ –∑–∞–≥—Ä—É–∑–∏—Ç—å, –ø—Ä–∏—á–∏–Ω–∞)
        """
        model = self.models.get(model_name)
        if not model:
            return False, f"Model {model_name} not found"

        available_ram = self._get_available_ram()
        loaded_ram = self._get_loaded_ram()

        # –£–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞?
        if model_name in self.loaded_models:
            return True, "Already loaded"

        # –•–≤–∞—Ç–∞–µ—Ç RAM?
        if loaded_ram + model.ram_gb <= available_ram:
            return True, "Sufficient RAM"

        # –ù—É–∂–Ω–æ –≤—ã–≥—Ä—É–∑–∏—Ç—å –¥—Ä—É–≥–∏–µ –º–æ–¥–µ–ª–∏
        return False, f"Need to unload models (need {model.ram_gb}GB, available {available_ram - loaded_ram}GB)"

    def suggest_unload(self, needed_ram: float) -> List[str]:
        """
        –ü—Ä–µ–¥–ª–æ–∂–∏—Ç—å –º–æ–¥–µ–ª–∏ –¥–ª—è –≤—ã–≥—Ä—É–∑–∫–∏

        Returns:
            –°–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–∂–Ω–æ –≤—ã–≥—Ä—É–∑–∏—Ç—å
        """
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ "–Ω–µ–Ω—É–∂–Ω–æ—Å—Ç–∏": –¥–∞–≤–Ω–æ –Ω–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å, –±–æ–ª—å—à–∏–µ
        candidates = []

        for name, model in self.loaded_models.items():
            if model.tier == ModelTier.FAST:
                # –ë—ã—Å—Ç—Ä—ã–µ –º–æ–¥–µ–ª–∏ –¥–µ—Ä–∂–∏–º
                continue
            candidates.append((name, model.ram_gb))

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–∞–∑–º–µ—Ä—É (—Å–Ω–∞—á–∞–ª–∞ –±–æ–ª—å—à–∏–µ)
        candidates.sort(key=lambda x: -x[1])

        to_unload = []
        freed = 0
        for name, ram in candidates:
            to_unload.append(name)
            freed += ram
            if freed >= needed_ram:
                break

        return to_unload

    def get_model_info(self, model_name: str) -> Optional[dict]:
        """–ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª–∏"""
        model = self.models.get(model_name)
        if not model:
            return None

        return {
            "name": model.name,
            "tier": model.tier.value,
            "ram_gb": model.ram_gb,
            "context_window": model.context_window,
            "strengths": model.strengths,
            "tokens_per_second": model.tokens_per_second,
            "is_loaded": model.is_loaded,
        }

    def get_status(self) -> dict:
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç—É—Å —Ä–æ—É—Ç–µ—Ä–∞"""
        available_ram = self._get_available_ram()
        loaded_ram = self._get_loaded_ram()

        return {
            "total_models": len(self.models),
            "loaded_models": list(self.loaded_models.keys()),
            "available_ram_gb": round(available_ram, 1),
            "loaded_ram_gb": round(loaded_ram, 1),
            "can_load_more": available_ram - loaded_ram > 5,
        }


# CLI –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    router = ModelRouter()

    print("üìä Model Router Status")
    print("=" * 50)

    status = router.get_status()
    print(f"Total models: {status['total_models']}")
    print(f"Available RAM: {status['available_ram_gb']} GB")

    print("\nüîç Task ‚Üí Model Mapping:")
    for task_type in ["architecture", "implementation", "review", "testing", "documentation"]:
        model, reason = router.select_model(task_type)
        print(f"  {task_type:15} ‚Üí {model.name:25} ({reason})")

    print("\nüì¶ Available Models:")
    for name, model in router.models.items():
        status_icon = "‚úÖ" if model.is_loaded else "‚¨ú"
        print(f"  {status_icon} {name:25} {model.ram_gb:5.1f}GB  {model.tier.value:10}  {model.strengths[:2]}")
