"""
Task Parser & Planner

–†–∞–∑–±–∏—Ä–∞–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∑–∞–ø—Ä–æ—Å –Ω–∞ –ø–æ–¥–∑–∞–¥–∞—á–∏ –∏ —Å—Ç—Ä–æ–∏—Ç –≥—Ä–∞—Ñ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π.
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç –±—ã—Å—Ç—Ä—É—é –º–æ–¥–µ–ª—å (qwen3:8b) –¥–ª—è –¥–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏–∏.
"""

import json
import re
from dataclasses import dataclass, field
from enum import Enum
from typing import List, Dict, Optional, Set
from datetime import datetime
import httpx


class TaskType(Enum):
    """–¢–∏–ø—ã –∑–∞–¥–∞—á"""
    ARCHITECTURE = "architecture"      # –ü—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ, ADR
    IMPLEMENTATION = "implementation"  # –ù–∞–ø–∏—Å–∞–Ω–∏–µ –∫–æ–¥–∞
    REFACTORING = "refactoring"       # –†–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥
    BUGFIX = "bugfix"                 # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –±–∞–≥–æ–≤
    TESTING = "testing"               # –ù–∞–ø–∏—Å–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤
    REVIEW = "review"                 # Code review
    DOCUMENTATION = "documentation"   # –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è
    DEVOPS = "devops"                 # CI/CD, –¥–µ–ø–ª–æ–π
    RESEARCH = "research"             # –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∫–æ–¥–æ–≤–æ–π –±–∞–∑—ã


class TaskPriority(Enum):
    """–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç—ã –∑–∞–¥–∞—á"""
    CRITICAL = 1
    HIGH = 2
    MEDIUM = 3
    LOW = 4


class TaskStatus(Enum):
    """–°—Ç–∞—Ç—É—Å—ã –∑–∞–¥–∞—á"""
    PENDING = "pending"
    IN_PROGRESS = "in_progress"
    COMPLETED = "completed"
    FAILED = "failed"
    BLOCKED = "blocked"


@dataclass
class Task:
    """–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –æ–¥–Ω–æ–π –∑–∞–¥–∞—á–∏"""
    id: str
    title: str
    description: str
    type: TaskType
    priority: TaskPriority = TaskPriority.MEDIUM
    status: TaskStatus = TaskStatus.PENDING

    # –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
    depends_on: List[str] = field(default_factory=list)
    blocks: List[str] = field(default_factory=list)

    # –ö–æ–Ω—Ç–µ–∫—Å—Ç
    files_to_read: List[str] = field(default_factory=list)
    files_to_modify: List[str] = field(default_factory=list)
    estimated_tokens: int = 0

    # –†–µ–∑—É–ª—å—Ç–∞—Ç
    result: Optional[str] = None
    error: Optional[str] = None

    # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    created_at: datetime = field(default_factory=datetime.now)
    started_at: Optional[datetime] = None
    completed_at: Optional[datetime] = None

    def is_ready(self, completed_tasks: Set[str]) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –≥–æ—Ç–æ–≤–∞ –ª–∏ –∑–∞–¥–∞—á–∞ –∫ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—é"""
        return all(dep in completed_tasks for dep in self.depends_on)

    def to_dict(self) -> dict:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ —Å–ª–æ–≤–∞—Ä—å"""
        return {
            "id": self.id,
            "title": self.title,
            "description": self.description,
            "type": self.type.value,
            "priority": self.priority.value,
            "status": self.status.value,
            "depends_on": self.depends_on,
            "blocks": self.blocks,
            "files_to_read": self.files_to_read,
            "files_to_modify": self.files_to_modify,
        }


@dataclass
class TaskGraph:
    """–ì—Ä–∞—Ñ –∑–∞–¥–∞—á —Å –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—è–º–∏"""
    tasks: Dict[str, Task] = field(default_factory=dict)
    root_task_id: Optional[str] = None

    def add_task(self, task: Task) -> None:
        """–î–æ–±–∞–≤–∏—Ç—å –∑–∞–¥–∞—á—É –≤ –≥—Ä–∞—Ñ"""
        self.tasks[task.id] = task

        # –û–±–Ω–æ–≤–∏—Ç—å –æ–±—Ä–∞—Ç–Ω—ã–µ —Å–≤—è–∑–∏
        for dep_id in task.depends_on:
            if dep_id in self.tasks:
                if task.id not in self.tasks[dep_id].blocks:
                    self.tasks[dep_id].blocks.append(task.id)

    def get_ready_tasks(self, completed: Set[str]) -> List[Task]:
        """–ü–æ–ª—É—á–∏—Ç—å –∑–∞–¥–∞—á–∏, –≥–æ—Ç–æ–≤—ã–µ –∫ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—é"""
        ready = []
        for task in self.tasks.values():
            if task.status == TaskStatus.PENDING and task.is_ready(completed):
                ready.append(task)

        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É
        ready.sort(key=lambda t: t.priority.value)
        return ready

    def get_execution_order(self) -> List[str]:
        """–¢–æ–ø–æ–ª–æ–≥–∏—á–µ—Å–∫–∞—è —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –¥–ª—è –ø–æ—Ä—è–¥–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è"""
        visited = set()
        order = []

        def dfs(task_id: str):
            if task_id in visited:
                return
            visited.add(task_id)

            task = self.tasks.get(task_id)
            if task:
                for dep_id in task.depends_on:
                    dfs(dep_id)
                order.append(task_id)

        for task_id in self.tasks:
            dfs(task_id)

        return order

    def to_mermaid(self) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è Mermaid –¥–∏–∞–≥—Ä–∞–º–º—ã"""
        lines = ["graph TD"]

        for task in self.tasks.values():
            # –£–∑–µ–ª
            status_emoji = {
                TaskStatus.PENDING: "‚è≥",
                TaskStatus.IN_PROGRESS: "üîÑ",
                TaskStatus.COMPLETED: "‚úÖ",
                TaskStatus.FAILED: "‚ùå",
                TaskStatus.BLOCKED: "üö´",
            }
            emoji = status_emoji.get(task.status, "")
            lines.append(f'    {task.id}["{emoji} {task.title}"]')

            # –°–≤—è–∑–∏
            for dep_id in task.depends_on:
                lines.append(f'    {dep_id} --> {task.id}')

        return "\n".join(lines)


class TaskParser:
    """
    –ü–∞—Ä—Å–µ—Ä –∑–∞–¥–∞—á —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º LLM –¥–ª—è –¥–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏–∏
    """

    DECOMPOSITION_PROMPT = '''–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –¥–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏–∏ –∑–∞–¥–∞—á —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏.

–†–∞–∑–±–µ–π —Å–ª–µ–¥—É—é—â—É—é –∑–∞–¥–∞—á—É –Ω–∞ –ø–æ–¥–∑–∞–¥–∞—á–∏. –î–ª—è –∫–∞–∂–¥–æ–π –ø–æ–¥–∑–∞–¥–∞—á–∏ —É–∫–∞–∂–∏:
- id: —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä (task_1, task_2, ...)
- title: –∫–æ—Ä–æ—Ç–∫–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ
- description: –ø–æ–¥—Ä–æ–±–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ —á—Ç–æ –Ω—É–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å
- type: —Ç–∏–ø –∑–∞–¥–∞—á–∏ (architecture, implementation, refactoring, bugfix, testing, review, documentation, devops, research)
- priority: –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç (1=critical, 2=high, 3=medium, 4=low)
- depends_on: —Å–ø–∏—Å–æ–∫ id –∑–∞–¥–∞—á, –æ—Ç –∫–æ—Ç–æ—Ä—ã—Ö –∑–∞–≤–∏—Å–∏—Ç —ç—Ç–∞ –∑–∞–¥–∞—á–∞

–ó–ê–î–ê–ß–ê: {task_description}

–ö–û–ù–¢–ï–ö–°–¢ –ü–†–û–ï–ö–¢–ê:
{project_context}

–û—Ç–≤–µ—Ç—å –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON:
```json
{{
  "workflow_type": "feature|bugfix|refactor|docs|test",
  "summary": "–∫—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –ø–ª–∞–Ω–∞",
  "tasks": [
    {{
      "id": "task_1",
      "title": "...",
      "description": "...",
      "type": "...",
      "priority": 2,
      "depends_on": []
    }}
  ]
}}
```

–í–∞–∂–Ω–æ:
- –ü–µ—Ä–≤–∞—è –∑–∞–¥–∞—á–∞ –æ–±—ã—á–Ω–æ research –∏–ª–∏ architecture
- Implementation –∑–∞–≤–∏—Å–∏—Ç –æ—Ç architecture
- Testing –∑–∞–≤–∏—Å–∏—Ç –æ—Ç implementation
- Review –∑–∞–≤–∏—Å–∏—Ç –æ—Ç implementation
- Documentation –º–æ–∂–µ—Ç –±—ã—Ç—å –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ'''

    CLASSIFICATION_PROMPT = '''–ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–π –∑–∞–¥–∞—á—É —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏.

–ó–ê–î–ê–ß–ê: {task_description}

–û–ø—Ä–µ–¥–µ–ª–∏:
1. workflow_type: feature|bugfix|refactor|docs|test
2. complexity: simple|medium|complex
3. estimated_tasks: –ø—Ä–∏–º–µ—Ä–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–¥–∑–∞–¥–∞—á (1-10)
4. needs_architecture: true/false ‚Äî –Ω—É–∂–µ–Ω –ª–∏ —ç—Ç–∞–ø –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
5. needs_testing: true/false ‚Äî –Ω—É–∂–Ω—ã –ª–∏ —Ç–µ—Å—Ç—ã
6. primary_language: –æ—Å–Ω–æ–≤–Ω–æ–π —è–∑—ã–∫ (python/javascript/go/etc)

–û—Ç–≤–µ—Ç—å –≤ JSON:
```json
{{
  "workflow_type": "feature",
  "complexity": "medium",
  "estimated_tasks": 4,
  "needs_architecture": true,
  "needs_testing": true,
  "primary_language": "python"
}}
```'''

    def __init__(
        self,
        ollama_url: str = "http://localhost:11434",
        fast_model: str = "codestral:latest",
        smart_model: str = "qwen3-coder:30b"
    ):
        self.ollama_url = ollama_url
        self.fast_model = fast_model
        self.smart_model = smart_model
        self.client = httpx.Client(timeout=120.0)

    def _call_llm(self, prompt: str, model: str = None) -> str:
        """–í—ã–∑–æ–≤ LLM —á–µ—Ä–µ–∑ Ollama API"""
        model = model or self.fast_model

        response = self.client.post(
            f"{self.ollama_url}/api/generate",
            json={
                "model": model,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": 0.3,
                    "num_predict": 2048,
                }
            }
        )
        response.raise_for_status()
        return response.json()["response"]

    def _extract_json(self, text: str) -> dict:
        """–ò–∑–≤–ª–µ—á—å JSON –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        # –ò—â–µ–º JSON –±–ª–æ–∫
        json_match = re.search(r'```json\s*(.*?)\s*```', text, re.DOTALL)
        if json_match:
            try:
                return json.loads(json_match.group(1))
            except json.JSONDecodeError:
                pass

        # –ü—Ä–æ–±—É–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç –∫–∞–∫ JSON
        try:
            return json.loads(text)
        except json.JSONDecodeError:
            pass

        # –ò—â–µ–º –ø–µ—Ä–≤—É—é { –∏ –ø–æ—Å–ª–µ–¥–Ω—é—é }
        start = text.find('{')
        end = text.rfind('}')
        if start != -1 and end != -1:
            try:
                return json.loads(text[start:end+1])
            except json.JSONDecodeError:
                pass

        return {}

    def classify_task(self, task_description: str) -> dict:
        """
        –ë—ã—Å—Ç—Ä–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∑–∞–¥–∞—á–∏

        Returns:
            dict —Å –ø–æ–ª—è–º–∏: workflow_type, complexity, estimated_tasks, etc.
        """
        prompt = self.CLASSIFICATION_PROMPT.format(
            task_description=task_description
        )

        response = self._call_llm(prompt, self.fast_model)
        result = self._extract_json(response)

        # –ó–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        defaults = {
            "workflow_type": "feature",
            "complexity": "medium",
            "estimated_tasks": 3,
            "needs_architecture": True,
            "needs_testing": True,
            "primary_language": "python"
        }

        return {**defaults, **result}

    def decompose_task(
        self,
        task_description: str,
        project_context: str = ""
    ) -> TaskGraph:
        """
        –î–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏—è –∑–∞–¥–∞—á–∏ –Ω–∞ –ø–æ–¥–∑–∞–¥–∞—á–∏

        Args:
            task_description: –û–ø–∏—Å–∞–Ω–∏–µ –∑–∞–¥–∞—á–∏ –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            project_context: –ö–æ–Ω—Ç–µ–∫—Å—Ç –ø—Ä–æ–µ–∫—Ç–∞ (—Å—Ç—Ä—É–∫—Ç—É—Ä–∞, –ø–∞—Ç—Ç–µ—Ä–Ω—ã)

        Returns:
            TaskGraph —Å –∑–∞–¥–∞—á–∞–º–∏ –∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—è–º–∏
        """
        prompt = self.DECOMPOSITION_PROMPT.format(
            task_description=task_description,
            project_context=project_context or "–ö–æ–Ω—Ç–µ–∫—Å—Ç –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω"
        )

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–º–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è –¥–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏–∏
        response = self._call_llm(prompt, self.smart_model)
        result = self._extract_json(response)

        # –°–æ–∑–¥–∞—ë–º –≥—Ä–∞—Ñ –∑–∞–¥–∞—á
        graph = TaskGraph()

        tasks_data = result.get("tasks", [])

        for task_data in tasks_data:
            task = Task(
                id=task_data.get("id", f"task_{len(graph.tasks)+1}"),
                title=task_data.get("title", "Untitled"),
                description=task_data.get("description", ""),
                type=TaskType(task_data.get("type", "implementation")),
                priority=TaskPriority(task_data.get("priority", 3)),
                depends_on=task_data.get("depends_on", []),
                files_to_read=task_data.get("files_to_read", []),
                files_to_modify=task_data.get("files_to_modify", []),
            )
            graph.add_task(task)

        # –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å root task
        if graph.tasks:
            # Root ‚Äî –∑–∞–¥–∞—á–∞ –±–µ–∑ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
            for task_id, task in graph.tasks.items():
                if not task.depends_on:
                    graph.root_task_id = task_id
                    break

        return graph

    def create_simple_task(
        self,
        task_description: str,
        task_type: TaskType = TaskType.IMPLEMENTATION
    ) -> TaskGraph:
        """
        –°–æ–∑–¥–∞—Ç—å –ø—Ä–æ—Å—Ç—É—é –∑–∞–¥–∞—á—É –±–µ–∑ –¥–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏–∏

        –î–ª—è –±—ã—Å—Ç—Ä—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π, –Ω–µ —Ç—Ä–µ–±—É—é—â–∏—Ö LLM
        """
        task = Task(
            id="task_1",
            title=task_description[:50],
            description=task_description,
            type=task_type,
            priority=TaskPriority.MEDIUM,
        )

        graph = TaskGraph()
        graph.add_task(task)
        graph.root_task_id = task.id

        return graph

    def estimate_complexity(self, graph: TaskGraph) -> dict:
        """
        –û—Ü–µ–Ω–∫–∞ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –ø–ª–∞–Ω–∞

        Returns:
            dict: {
                "total_tasks": int,
                "parallel_groups": int,
                "estimated_time_minutes": int,
                "models_needed": list
            }
        """
        total = len(graph.tasks)

        # –ì—Ä—É–ø–ø—ã –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á
        order = graph.get_execution_order()
        groups = []
        current_group = []
        completed = set()

        for task_id in order:
            task = graph.tasks[task_id]
            if task.is_ready(completed):
                current_group.append(task_id)
            else:
                if current_group:
                    groups.append(current_group)
                    completed.update(current_group)
                current_group = [task_id]

        if current_group:
            groups.append(current_group)

        # –ú–æ–¥–µ–ª–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –∑–∞–¥–∞—á
        models_map = {
            TaskType.ARCHITECTURE: "deepseek-r1:32b",
            TaskType.IMPLEMENTATION: "qwen3-coder:30b",
            TaskType.REFACTORING: "qwen3-coder:30b",
            TaskType.BUGFIX: "qwen3-coder:30b",
            TaskType.TESTING: "qwen3-coder:30b",
            TaskType.REVIEW: "deepseek-r1:32b",
            TaskType.DOCUMENTATION: "qwen3:8b",
            TaskType.DEVOPS: "qwen3-coder:30b",
            TaskType.RESEARCH: "deepseek-r1:32b",
        }

        models_needed = list(set(
            models_map.get(t.type, "qwen3-coder:30b")
            for t in graph.tasks.values()
        ))

        # –ü—Ä–∏–º–µ—Ä–Ω–æ–µ –≤—Ä–µ–º—è (2-5 –º–∏–Ω—É—Ç –Ω–∞ –∑–∞–¥–∞—á—É)
        estimated_time = total * 3

        return {
            "total_tasks": total,
            "parallel_groups": len(groups),
            "execution_groups": groups,
            "estimated_time_minutes": estimated_time,
            "models_needed": models_needed,
        }


# CLI –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    import sys

    parser = TaskParser()

    if len(sys.argv) > 1:
        task_desc = " ".join(sys.argv[1:])
    else:
        task_desc = "–î–æ–±–∞–≤–∏—Ç—å –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—é —Å JWT —Ç–æ–∫–µ–Ω–∞–º–∏"

    print(f"üìù –ó–∞–¥–∞—á–∞: {task_desc}\n")

    # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
    print("üîç –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è...")
    classification = parser.classify_task(task_desc)
    print(f"   –¢–∏–ø: {classification['workflow_type']}")
    print(f"   –°–ª–æ–∂–Ω–æ—Å—Ç—å: {classification['complexity']}")
    print(f"   –ü—Ä–∏–º–µ—Ä–Ω–æ –∑–∞–¥–∞—á: {classification['estimated_tasks']}")

    # –î–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏—è
    print("\nüìã –î–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏—è...")
    graph = parser.decompose_task(task_desc)

    print(f"\nüìä –°–æ–∑–¥–∞–Ω–æ –∑–∞–¥–∞—á: {len(graph.tasks)}")
    for task in graph.tasks.values():
        deps = f" (depends: {task.depends_on})" if task.depends_on else ""
        print(f"   [{task.type.value}] {task.title}{deps}")

    # –û—Ü–µ–Ω–∫–∞
    estimate = parser.estimate_complexity(graph)
    print(f"\n‚è±Ô∏è –û—Ü–µ–Ω–∫–∞:")
    print(f"   –ì—Ä—É–ø–ø –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {estimate['parallel_groups']}")
    print(f"   –ü—Ä–∏–º–µ—Ä–Ω–æ–µ –≤—Ä–µ–º—è: {estimate['estimated_time_minutes']} –º–∏–Ω")
    print(f"   –ú–æ–¥–µ–ª–∏: {estimate['models_needed']}")

    # Mermaid –¥–∏–∞–≥—Ä–∞–º–º–∞
    print(f"\nüìà –î–∏–∞–≥—Ä–∞–º–º–∞:\n{graph.to_mermaid()}")
